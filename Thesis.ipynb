{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyS7_YA69D5L",
        "outputId": "6c2891a8-caee-4ccc-ccba-c8fb3c961ff0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.__version__: 2.10.1\n",
            "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "#setup\n",
        "\n",
        "# digit 8\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "# %tensorflow_version 1.x\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import cv2\n",
        "import scipy\n",
        "import scipy.spatial\n",
        "from scipy import ndimage\n",
        "from skimage import metrics\n",
        "import tensorflow as tf\n",
        "\n",
        "print('tf.__version__: {}'.format(tf.__version__))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import datasets, layers, models\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "tf.config.set_visible_devices([], 'GPU')\n",
        "\n",
        "print(tf.config.list_physical_devices())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dY4k2la_m8Ht"
      },
      "outputs": [],
      "source": [
        "def generate_translation_matrices(image_size, stride, im_range):\n",
        "\n",
        "    # Generate all the translations in x and y directions\n",
        "\n",
        "    translations_x, translations_y = tf.meshgrid(tf.range(-image_size/2/im_range, image_size/2/im_range, stride),\n",
        "                                                 tf.range(-image_size/2/im_range, image_size/2/im_range, stride))\n",
        "    # Convert the translations to float32\n",
        "    translations_x = tf.cast(translations_x, tf.float32)\n",
        "    translations_y = tf.cast(translations_y, tf.float32)\n",
        "\n",
        "    # Reshape the translations to be able to concatenate them\n",
        "    translations_x = tf.reshape(translations_x, [-1, 1])\n",
        "    translations_y = tf.reshape(translations_y, [-1, 1])\n",
        "\n",
        "    # Create the 2D affine matrix for all translations\n",
        "    num_matrices = tf.size(translations_x)\n",
        "    ones = tf.ones((num_matrices, 1), dtype=tf.float32)\n",
        "    zeros = tf.zeros((num_matrices, 1), dtype=tf.float32)\n",
        "\n",
        "    matrices = tf.concat([ones, zeros, translations_x, zeros, ones, translations_y, zeros, zeros], axis=1)\n",
        "\n",
        "    return matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r2g841hXnNSQ"
      },
      "outputs": [],
      "source": [
        "def prepare_image_for_transform(image, num_transforms):\n",
        "    image = tf.expand_dims(image, axis=0)  # Add batch dimension\n",
        "    image = tf.tile(image, [num_transforms, 1, 1])  # Repeat the image for all transformations\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kx71id2RnQHS"
      },
      "outputs": [],
      "source": [
        "def apply_all_transforms(image, matrices):\n",
        "    # Prepare the image for transformation\n",
        "    num_transforms = tf.shape(matrices)[0]\n",
        "    image_shape = tf.shape(image)\n",
        "    image = prepare_image_for_transform(image, num_transforms)\n",
        "    image = tf.expand_dims(image, axis=-1)\n",
        "    # Apply all transforms\n",
        "    transformed_images = tf.raw_ops.ImageProjectiveTransformV3(images=image,\n",
        "                                                               transforms=matrices,\n",
        "                                                               output_shape=image_shape,\n",
        "                                                               interpolation='NEAREST',\n",
        "                                                               fill_value=0.0)\n",
        "    return transformed_images.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wEE0f-QrnYy1"
      },
      "outputs": [],
      "source": [
        "def generate_perturbations(image, stride, shape, im_range):\n",
        "    # Generate all the transformations\n",
        "    matrices = generate_translation_matrices(shape, stride, im_range)\n",
        "    # Apply all the transformations\n",
        "    transformed_images = apply_all_transforms(image, matrices)\n",
        "    # Return the transformations\n",
        "    return transformed_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-F9mENsEndpi"
      },
      "outputs": [],
      "source": [
        "def mse(imageA, imageB):\n",
        "    err = np.sum((imageA - imageB) ** 2)\n",
        "    err /= float(imageA.shape[0] * imageA.shape[1])\n",
        "    # return the MSE, the lower the error, the more \"similar\"\n",
        "    return err"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hGXL-iaSng9W"
      },
      "outputs": [],
      "source": [
        "def compare_images(imageA, imageB):\n",
        "    imageA = imageA.numpy()\n",
        "    imageA = imageA.astype(\"float\") / 255.0\n",
        "    imageB = imageB.astype(\"float\") / 255.0\n",
        "\n",
        "    m = mse(imageA, imageB)\n",
        "    s = metrics.structural_similarity(imageA, imageB, data_range=imageA.max() - imageA.min())\n",
        "    s_percentage = (s + 1) / 2 * 100\n",
        "\n",
        "    return m, s_percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ya8tKiIjnlPd"
      },
      "outputs": [],
      "source": [
        "# Assumes the patch consists of last 2 dimensions\n",
        "\n",
        "def get_LBM(x):\n",
        "\n",
        "    n_row = x.shape[-2]\n",
        "    n_col = x.shape[-1]\n",
        "\n",
        "    # Convert the patches to column vectors\n",
        "    x_col_shape = tf.shape(x).numpy()\n",
        "    x_col_shape[-2] = n_row * n_col\n",
        "    x_col_shape[-1] = 1\n",
        "    x_col = tf.reshape(x, x_col_shape)\n",
        "    x_col = tf.cast(x_col, tf.float32)\n",
        "    # print('x_col.shape', x_col.shape)\n",
        "\n",
        "    # Convert the patches to row vectors\n",
        "    x_row_shape = tf.shape(x).numpy()\n",
        "    x_row_shape[-2] = 1\n",
        "    x_row_shape[-1] = n_row * n_col\n",
        "    x_row = tf.reshape(x, x_row_shape)\n",
        "    x_row = tf.cast(x_row, tf.float32)\n",
        "    # print('x_row.shape', x_row.shape)\n",
        "\n",
        "    # Calculate the AdjM\n",
        "    sqr_diff = tf.math.sqrt(tf.math.abs(x_col - x_row))\n",
        "    # print('sqr_diff.shape', sqr_diff.shape)\n",
        "\n",
        "    Dismean = np.zeros([n_row * n_col, 2])\n",
        "    i = np.broadcast_to(np.arange(n_row).reshape([n_row, 1]), [n_row, n_col]).reshape(n_row * n_col)\n",
        "    j = np.broadcast_to(np.arange(n_col).reshape([1, n_col]), [n_row, n_col]).reshape(n_row * n_col)\n",
        "\n",
        "    Dismean[:, 0] = i\n",
        "    Dismean[:, 1] = j\n",
        "    # print('Dis')\n",
        "    # print(Dismean)\n",
        "\n",
        "    E = scipy.spatial.distance.cdist(Dismean, Dismean, 'euclidean')\n",
        "    E = E + np.identity(n_row * n_col)\n",
        "    # print('E.shape', E.shape)\n",
        "    # print(E[:, 0])\n",
        "\n",
        "    AdjM = tf.divide(sqr_diff, E)\n",
        "    # print('AdjM')\n",
        "    # print(AdjM.shape)\n",
        "\n",
        "    # Sum the rows or cols of the AdjM (AdjM is symmetric so anything works)\n",
        "    sum_AdjM = tf.reduce_sum(AdjM, axis=-1)  # Sum cols\n",
        "    # print('sum_AdjM.shape', sum_AdjM.shape)\n",
        "    # print(sum_AdjM[0])\n",
        "\n",
        "    diagonal = tf.linalg.diag(sum_AdjM)\n",
        "    # print('diagonal.shape', diagonal.shape)\n",
        "\n",
        "    # LBM\n",
        "    LBM = AdjM - diagonal\n",
        "    # print('LBM.shape', LBM.shape)\n",
        "\n",
        "    return tf.cast(LBM, tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "f3GNAKjdn4q9"
      },
      "outputs": [],
      "source": [
        "class SVLayer(layers.Layer):\n",
        "\n",
        "    def __init__(self, patch_size, stride, patch_size_2, stride_2, **kwargs):\n",
        "        super(SVLayer, self).__init__(**kwargs)\n",
        "        self.patch_size = patch_size\n",
        "        self.stride = stride\n",
        "\n",
        "        self.patch_size_2 = patch_size_2\n",
        "        self.stride_2 = stride_2\n",
        "\n",
        "    def call(self, inputs):\n",
        "        n_images = inputs.shape[0]\n",
        "        # print('input', inputs.shape)\n",
        "\n",
        "        # Get BIG patches from the input batch of images\n",
        "        big_patches = tf.image.extract_patches(inputs, sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "                                               strides=[1, self.stride, self.stride, 1], rates=[1, 1, 1, 1],\n",
        "                                               padding='VALID')\n",
        "\n",
        "        # print('big_patches.shape', big_patches.shape)\n",
        "        _, n_big_row, n_big_col, _ = big_patches.shape\n",
        "        n_big = n_big_row * n_big_col  # No. of big patches\n",
        "\n",
        "        ## tf.extract_image_patches flattens the patch in extraction.\n",
        "        ## We have to reshape the patches to (patch_size x patch_size) reshape\n",
        "        big_patches = tf.reshape(big_patches,\n",
        "                                 (n_images, n_big, self.patch_size, self.patch_size))\n",
        "\n",
        "        # print('big_patches.shape before transpose', big_patches.shape)  ###input_patches_reshape.shape (None, 3, 3, 5, 5)\n",
        "        big_patches = tf.transpose(big_patches, [0, 2, 3, 1])\n",
        "        # print('big_patches.shape after transpose', big_patches.shape)  ###input_patches_reshape.shape (None, 3, 3, 5, 5)\n",
        "\n",
        "        ## Now we need to extract smaller patches from the above big patch\n",
        "        small_patches = tf.image.extract_patches(big_patches, sizes=[1, self.patch_size_2, self.patch_size_2, 1], \\\n",
        "                                                 strides=[1, self.stride_2, self.stride_2, 1],\n",
        "                                                 rates=[1, 1, 1, 1], padding='VALID')\n",
        "\n",
        "        _, n_small_row, n_small_col, _ = small_patches.shape\n",
        "        n_small = n_small_row * n_small_col  # No. of small patches\n",
        "        # print('small_patches.shape', small_patches.shape)\n",
        "\n",
        "        # The last axis contains the small_patch at a particular posiiton for each of the n_big big_patches\n",
        "        # and there are n_small positons.\n",
        "        # So we need to breack the last axis into pieces of size = self.patch_size_2 ** 2\n",
        "\n",
        "        small_patches = tf.reshape(small_patches,\n",
        "                                   (n_images, n_small_row, n_small_col, self.patch_size_2 * self.patch_size_2, n_big))\n",
        "        small_patches = tf.reshape(small_patches, (\n",
        "        n_images, n_small_row, n_small_col, self.patch_size_2, self.patch_size_2, n_big_row, n_big_col))\n",
        "        # print('small_patches.shape before transpose', small_patches.shape)\n",
        "\n",
        "        small_patches = tf.transpose(small_patches, [0, 5, 6, 1, 2, 3, 4])\n",
        "        # print('small_patches.shape after transpose', small_patches.shape)\n",
        "\n",
        "        # Make the aggregate patch by averaging the small patches\n",
        "        # There is one aggregate patch for each big patch\n",
        "\n",
        "        agg_patches = tf.math.reduce_mean(small_patches, axis=(5, 6))\n",
        "        agg_patches = tf.cast(agg_patches, tf.float32)\n",
        "        # print('agg_patches.shape', agg_patches.shape)\n",
        "\n",
        "        agg_patches = tf.reshape(agg_patches, (n_images, n_big_row, n_big_col, n_small_row, n_small_col))\n",
        "        # print('agg_patches.shape', agg_patches.shape)\n",
        "\n",
        "        agg_LBM = get_LBM(agg_patches)\n",
        "        agg_sv, agg_eigvecs = tf.linalg.eigh(agg_LBM)  # Trunk eigenvalue vec\n",
        "        # print('agg_sv.shape', agg_sv.shape)\n",
        "\n",
        "        # Calculate the sampler vector of small_patches\n",
        "        small_LBM = get_LBM(small_patches)\n",
        "        small_sv, small_eigvecs = tf.linalg.eigh(small_LBM)\n",
        "\n",
        "        #print('small_sv.shape', small_sv.shape)\n",
        "        # print(np.max(np.sum(small_sv[0], axis=(2, 3, 4))), np.min(np.sum(small_sv[1], axis=(2, 3, 4))))\n",
        "\n",
        "        # Replicate the aggregaate patches for each of the small patch\n",
        "        agg_sv = tf.expand_dims(agg_sv, axis=3)\n",
        "        agg_sv = tf.expand_dims(agg_sv, axis=3)\n",
        "        agg_sv = tf.repeat(agg_sv, repeats=n_small_row, axis=3)\n",
        "        agg_sv = tf.repeat(agg_sv, repeats=n_small_col, axis=4)\n",
        "        #print('agg_sv.shape after repeat', agg_sv.shape)\n",
        "\n",
        "        # print('agg', tf.math.reduce_max(agg_sv), tf.math.reduce_min(agg_sv))\n",
        "        # print('small', tf.math.reduce_max(small_sv), tf.math.reduce_min(small_sv))\n",
        "\n",
        "        # Concatenate the aggeregate SVs and small SVs\n",
        "        sv = tf.concat([agg_sv, small_sv], axis=-1)\n",
        "        #print('sv.shape', sv.shape)\n",
        "\n",
        "        return sv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "juWJd1WaoXvC",
        "outputId": "bf80b7f2-bc5d-40ee-baa0-d7b67d3fc4b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NUM IMAGES GENERATED:  784\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"for class_no in range(NUM_CLASSES):\\n    image = train_images[train_labels == class_no][0]\\n    initial_fig.add_subplot(10, 1, class_no + 1)\\n    image = tf.squeeze(image)\\n    plt.imshow(image)\\n    plt.axis('off')\\n    plt.title('Class: {}'.format(class_no))\""
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1280x2048 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# from SVLayer import *\n",
        "# from SVLayer2 import *\n",
        "\n",
        "# Load MNIST datasets provided by TensorFlow\n",
        "(train_images, train_labels), (_, _) = datasets.mnist.load_data()\n",
        "\n",
        "# train_images = train_images[:10]\n",
        "# train_labels = train_labels[:10]\n",
        "# test_images = test_images[:10]\n",
        "# test_labels = test_labels[:10]\n",
        "\n",
        "NUM_TRAIN, H, W = train_images.shape\n",
        "# NUM_TEST, _, _ = test_images.shape\n",
        "NUM_CLASSES = np.max(train_labels) + 1\n",
        "\n",
        "NUM_IMAGES = 200  # No. images per class\n",
        "FINAL_TEST_CLASS = 8\n",
        "SCALE = int(12 * 28 / H)\n",
        "new_train_images = []\n",
        "new_train_labels = []\n",
        "TRAIN_CLASS = 2\n",
        "class_images = train_images[train_labels == TRAIN_CLASS][:NUM_IMAGES]\n",
        "class_labels = train_labels[train_labels == TRAIN_CLASS][:NUM_IMAGES]\n",
        "new_train_images.append(class_images)\n",
        "new_train_labels.append(class_labels)\n",
        "\n",
        "# for i in range(NUM_CLASSES):\n",
        "#    class_images = train_images[train_labels == i][:NUM_IMAGES]\n",
        "#    class_labels = train_labels[train_labels == i][:NUM_IMAGES]\n",
        "#\n",
        "#    new_train_images.append(class_images)\n",
        "#    new_train_labels.append(class_labels)\n",
        "\n",
        "input_image = ndimage.zoom(train_images[train_labels == FINAL_TEST_CLASS][0].reshape((-1, H, W, 1)),\n",
        "                           zoom=[1, SCALE, SCALE, 1])\n",
        "\n",
        "new_train_images = np.concatenate(new_train_images, axis=0)\n",
        "new_train_labels = np.concatenate(new_train_labels, axis=0)\n",
        "\n",
        "train_images = new_train_images\n",
        "train_labels = new_train_labels\n",
        "\n",
        "stride   = 1\n",
        "im_range = 1  # Only cover half  the image (1/4 on each side)\n",
        "NUM_IMAGES = int(np.ceil(H / stride / im_range) * np.ceil(H / stride / im_range))\n",
        "print(\"NUM IMAGES GENERATED: \", NUM_IMAGES)\n",
        "\n",
        "train_images = generate_perturbations(train_images[0], stride, H, im_range)\n",
        "train_images = ndimage.zoom(train_images.reshape((-1, H, W, 1)), zoom=[1, SCALE, SCALE, 1])\n",
        "# test_images = ndimage.zoom(test_images.reshape((NUM_TEST, H, W, 1)), zoom=[1, SCALE, SCALE, 1])\n",
        "original_imgs = np.squeeze(train_images)\n",
        "\n",
        "H = int(H * SCALE)\n",
        "W = int(W * SCALE)\n",
        "\n",
        "# Check initial images\n",
        "initial_fig = plt.figure(figsize=(10, 16), dpi=128)\n",
        "\n",
        "\"\"\"for class_no in range(NUM_CLASSES):\n",
        "    image = train_images[train_labels == class_no][0]\n",
        "    initial_fig.add_subplot(10, 1, class_no + 1)\n",
        "    image = tf.squeeze(image)\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.title('Class: {}'.format(class_no))\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "YDGfVGXso4qP",
        "outputId": "505bb1ac-2565-4719-8433-c40886b02039"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original_imgs (784, 336, 336)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\" ### print the rotating training pairs\\n\\nfor i in range(NUM_IMAGES):\\n\\n    input_img_fig.add_subplot(NUM_IMAGES, 2, 2 * i + 1)\\n    original_imgs = tf.squeeze(original_imgs)\\n\\n    plt.imshow(original_imgs[i])\\n    plt.axis('off')\\n    plt.title('Original')\\n\\n    input_img_fig.add_subplot(NUM_IMAGES, 2, 2 * i + 2)\\n    rotated_imgs = tf.squeeze(rotated_imgs)\\n\\n    plt.imshow(rotated_imgs[i])\\n    plt.axis('off')\\n    plt.title('Rotated')\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1600x3200 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Rotation Experiment\n",
        "# Given multiple pairs of images, generate a mapping using some similarity measure\n",
        "# applied to eigvectors of LBM of patches of the two images\n",
        "\n",
        "# 1. Get an image of class == TEST_CLASS and rotate a copy of it\n",
        "#TEST_CLASS = 3\n",
        "#original_imgs = np.squeeze(train_images[train_labels == TEST_CLASS][:NUM_IMAGES])\n",
        "\n",
        "if NUM_IMAGES == 1:\n",
        "    original_imgs = np.expand_dims(original_imgs, axis=0)\n",
        "\n",
        "print('original_imgs', original_imgs.shape)  # original_imgs (20, 336, 336)\n",
        "\n",
        "rotation_angle = 45\n",
        "translation_x = 0\n",
        "translation_y = 0\n",
        "\n",
        "rotation_angle = np.deg2rad(rotation_angle)\n",
        "array_shape = original_imgs[0].shape\n",
        "center = np.array(array_shape) / 2.0 - 0.5\n",
        "translate_to_origin = np.array([\n",
        "    [1, 0, -center[1]],\n",
        "    [0, 1, -center[0]],\n",
        "    [0, 0, 1]\n",
        "])\n",
        "\n",
        "rotation = np.array([\n",
        "    [np.cos(rotation_angle), -np.sin(rotation_angle),  0],\n",
        "    [np.sin(rotation_angle),  np.cos(rotation_angle),  0],\n",
        "    [                     0,                       0,  1]\n",
        "])\n",
        "\n",
        "translate_back = np.array([\n",
        "    [1, 0, center[1] + translation_x],\n",
        "    [0, 1, center[0] + translation_y],\n",
        "    [0, 0, 1]\n",
        "])\n",
        "\n",
        "transform = np.dot(translate_back, np.dot(rotation, translate_to_origin))\n",
        "rotated_imgs = []\n",
        "\n",
        "for i in range(NUM_IMAGES):\n",
        "    original_img = original_imgs[i]\n",
        "    # print('original_img', original_img.shape)\n",
        "    # Control23 ????\n",
        "    array = np.array(original_img)\n",
        "    # height, width = array.shape[1,2]\n",
        "    # print('array.shape',array.shape)\n",
        "\n",
        "    rotated_img = ndimage.affine_transform(array,\n",
        "                                           transform,\n",
        "                                           output_shape=(SCALE * 28, SCALE * 28),\n",
        "                                           mode='nearest',\n",
        "                                           cval=0.0)\n",
        "    rotated_imgs.append(rotated_img)\n",
        "\n",
        "\"\"\"ROTATION = 45\n",
        "\n",
        "rotated_imgs = []\n",
        "\n",
        "for i in range(NUM_IMAGES):\n",
        "    original_img = original_imgs[i]\n",
        "    rotated_img = ndimage.rotate(original_img, ROTATION, reshape=False, cval=0)\n",
        "    rotated_imgs.append(rotated_img)\n",
        "\n",
        "rotated_imgs = np.array(rotated_imgs)\"\"\"\n",
        "\n",
        "# np.array([ndimage.rotate(original_img, ROTATION, reshape=False, cval=0) for original_img in original_img])\n",
        "\n",
        "# print('rotated_imgs', rotated_imgs.shape)\n",
        "\n",
        "\n",
        "\n",
        "input_img_fig = plt.figure(figsize=(16, 32), dpi=100)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" ### print the rotating training pairs\n",
        "\n",
        "for i in range(NUM_IMAGES):\n",
        "\n",
        "    input_img_fig.add_subplot(NUM_IMAGES, 2, 2 * i + 1)\n",
        "    original_imgs = tf.squeeze(original_imgs)\n",
        "\n",
        "    plt.imshow(original_imgs[i])\n",
        "    plt.axis('off')\n",
        "    plt.title('Original')\n",
        "\n",
        "    input_img_fig.add_subplot(NUM_IMAGES, 2, 2 * i + 2)\n",
        "    rotated_imgs = tf.squeeze(rotated_imgs)\n",
        "\n",
        "    plt.imshow(rotated_imgs[i])\n",
        "    plt.axis('off')\n",
        "    plt.title('Rotated')\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "va0vnPZipWUK"
      },
      "outputs": [],
      "source": [
        "# # 2. Generate the Mapping between Patches of original and new image\n",
        "\n",
        "def get_mapping(similarities):\n",
        "\n",
        "    # 1. Calculate row-wise softmax, to interpret the similarities as probabilities\n",
        "    #exp_similarities = np.exp(similarities/200)\n",
        "    #exp_similarities = np.exp(similarities)\n",
        "    #soft_similarities = exp_similarities / np.sum(exp_similarities, axis=1, keepdims=True)\n",
        "    soft_similarities = tf.nn.softmax(similarities, axis=1)\n",
        "\n",
        "    # 2. Sort rows according to entropy\n",
        "    sorted_entropy_idx = np.argsort(- tf.math.reduce_sum(soft_similarities * np.log(soft_similarities + CORRECTION), axis=1))\n",
        "    # print('entropy_idx', sorted_entropy_idx)\n",
        "\n",
        "    # 3. Generate mapping\n",
        "\n",
        "    # Select the most probable matching rotated patch for each row (original patch)\n",
        "    # starting from the row with least entropy (most confident)\n",
        "\n",
        "    bool_mapping = np.zeros_like(similarities,dtype=np.uint8)\n",
        "    #print('bool_mapping', bool_mapping.shape)\n",
        "    frozen = np.ones((similarities.shape[1]), dtype=np.uint8)\n",
        "\n",
        "    #print('frozen', frozen.shape)\n",
        "\n",
        "    for original_patch in tqdm(sorted_entropy_idx, desc=\"Patch Mapping\", bar_format='{l_bar}{bar}|'):\n",
        "\n",
        "        # print('original_patch', original_patch)\n",
        "\n",
        "        similarity = soft_similarities[original_patch]\n",
        "        similarity = np.multiply(frozen, similarity)\n",
        "        # print('similarity', similarity, similarity.max(), similarity.argmax())\n",
        "\n",
        "        sorted_similarity_idx = np.argsort(-similarity)  # Sort in Descending order\n",
        "        # print('sorted_similarity_idx', sorted_similarity_idx)\n",
        "\n",
        "        #i = 0\n",
        "\n",
        "        #matching_rotated_patch = sorted_similarity_idx[i]\n",
        "        #while frozen[matching_rotated_patch] == 1:\n",
        "        #    i += 1\n",
        "        #    matching_rotated_patch = sorted_similarity_idx[i]\n",
        "\n",
        "        frozen[sorted_similarity_idx[0]] = 0\n",
        "        # print('matching_rotated_patch', matching_rotated_patch)\n",
        "        bool_mapping[original_patch, sorted_similarity_idx[0]] = 1\n",
        "\n",
        "    mapping = np.argmax(bool_mapping, axis=1)\n",
        "\n",
        "    return mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "leHxv01tpugR"
      },
      "outputs": [],
      "source": [
        "def apply_mapping(input_img, mapping, params):\n",
        "\n",
        "    output_img = np.zeros_like(input_img)\n",
        "    NUM_PATCHES = params['NUM_PATCHES']\n",
        "\n",
        "    BIG_X_PATCHES = params['BIG_X_PATCHES']\n",
        "    BIG_Y_PATCHES = params['BIG_Y_PATCHES']\n",
        "\n",
        "    SMALL_X_PATCHES = params['SMALL_X_PATCHES']\n",
        "    SMALL_Y_PATCHES = params['SMALL_Y_PATCHES']\n",
        "\n",
        "    SMALL_PATCHES = params['SMALL_PATCHES']\n",
        "\n",
        "    PATCH_SIZE = params['PATCH_SIZE']\n",
        "    PATCH_SIZE_2 = params['PATCH_SIZE_2']\n",
        "\n",
        "    STRIDE = params['STRIDE']\n",
        "    STRIDE_2 = params['STRIDE_2']\n",
        "\n",
        "    for in_idx in range(NUM_PATCHES):\n",
        "        # Calculate the start and end rows and columns of patch in input image\n",
        "        big_in_idx = in_idx // SMALL_PATCHES\n",
        "\n",
        "        big_in_row = big_in_idx // BIG_Y_PATCHES\n",
        "        big_in_col = big_in_idx % BIG_Y_PATCHES\n",
        "\n",
        "        small_in_idx = in_idx % SMALL_PATCHES\n",
        "\n",
        "        small_in_row = small_in_idx // SMALL_Y_PATCHES\n",
        "        small_in_col = small_in_idx % SMALL_Y_PATCHES\n",
        "\n",
        "        in_row_start = big_in_row * STRIDE + small_in_row * STRIDE_2\n",
        "        in_row_end = in_row_start + PATCH_SIZE_2\n",
        "        in_col_start = big_in_col * STRIDE + small_in_col * STRIDE_2\n",
        "        in_col_end = in_col_start + PATCH_SIZE_2\n",
        "\n",
        "        # Calculate the start and end rows and columns of patch in mapped image\n",
        "        out_idx = mapping[in_idx]\n",
        "\n",
        "        \"\"\"out_idx = mapping[out_idx]\n",
        "        out_idx = mapping[out_idx]\n",
        "        out_idx = mapping[out_idx]\n",
        "        out_idx = mapping[out_idx]\n",
        "        out_idx = mapping[out_idx]\"\"\"\n",
        "\n",
        "        big_out_idx = out_idx // SMALL_PATCHES\n",
        "        big_out_row = big_out_idx // BIG_Y_PATCHES\n",
        "        big_out_col = big_out_idx % BIG_Y_PATCHES\n",
        "\n",
        "        small_out_idx = out_idx % SMALL_PATCHES\n",
        "        small_out_row = small_out_idx // SMALL_Y_PATCHES\n",
        "        small_out_col = small_out_idx % SMALL_Y_PATCHES\n",
        "\n",
        "        out_row_start = big_out_row * STRIDE + small_out_row * STRIDE_2\n",
        "        out_row_end = out_row_start + PATCH_SIZE_2\n",
        "        out_col_start = big_out_col * STRIDE + small_out_col * STRIDE_2\n",
        "        out_col_end = out_col_start + PATCH_SIZE_2\n",
        "\n",
        "        #output_img[out_row_start:out_row_end, out_col_start:out_col_end] = \\ input_img[in_row_start:in_row_end, in_col_start:in_col_end] ????\n",
        "\n",
        "        output_img[out_row_start:out_row_end, out_col_start:out_col_end] = input_img[in_row_start:in_row_end, in_col_start:in_col_end]\n",
        "\n",
        "        # ndimage.rotate(input_img[in_row_start:in_row_end, in_col_start:in_col_end], ROTATION, reshape=False, cval=0)\n",
        "        # input_img[in_row * patch_size:(in_row + 1) * patch_size, in_col * patch_size:(in_col + 1) * patch_size]\n",
        "\n",
        "    return output_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MfR7PmPqOey",
        "outputId": "023099d1-51fa-4870-a487-0bcc390d81af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "H, W: 336 336\n"
          ]
        }
      ],
      "source": [
        "CORRECTION = 1e-8\n",
        "PATCH_SIZE = 8  # To Match original SV implementation this patch size must be original patch size\n",
        "STRIDE = 4\n",
        "\n",
        "PATCH_SIZE_2 = 4  # this patch size must be same as above and stride = 1 (stride doesnt matter because patch sizes are same as big patches)\n",
        "STRIDE_2 = 4\n",
        "\n",
        "print('H, W:', H, W)\n",
        "\n",
        "BIG_X_PATCHES = (H - PATCH_SIZE) // STRIDE + 1\n",
        "BIG_Y_PATCHES = (W - PATCH_SIZE) // STRIDE + 1\n",
        "\n",
        "BIG_PATCHES = BIG_X_PATCHES * BIG_Y_PATCHES\n",
        "\n",
        "SMALL_X_PATCHES = (PATCH_SIZE - PATCH_SIZE_2) // STRIDE_2 + 1\n",
        "SMALL_Y_PATCHES = (PATCH_SIZE - PATCH_SIZE_2) // STRIDE_2 + 1\n",
        "\n",
        "SMALL_PATCHES = SMALL_X_PATCHES * SMALL_Y_PATCHES\n",
        "\n",
        "NUM_PATCHES = BIG_PATCHES * SMALL_PATCHES\n",
        "\n",
        "#######print('BIG_PATCHES: ', BIG_PATCHES)\n",
        "#######print('SMALL_PATCHES: ', SMALL_PATCHES)\n",
        "#######print('NUM_PATCHES: ', NUM_PATCHES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yjcdvOdUqUKp"
      },
      "outputs": [],
      "source": [
        "# A. Find eigen values for each patch in original and rotated image\n",
        "\n",
        "# Generate eigenvalues for the original image\n",
        "#######print('rotated_eigvals', rotated_eigvals.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "4d9e6c8f451f43099c7f74abb6ad0eff",
            "e7a07c3bcfcf443699ba77f00a13cb14",
            "dca3ba79a4cf4dafb15c82a01f2afdfa",
            "e8bc24bfbfa342169755c8a8655f8ced",
            "27dd6b05b425425faaf154ebf5fc4e60",
            "529f1b4cac4c45d797d47ba1e080b7ea",
            "8284d15c5df24c74b8b3d80768a675f1",
            "67f2015f06e84ab88f7d34baad053af0",
            "95234a15648141f8a1e5edb04b968119",
            "16aadefb829a415e8425985dee3714a3",
            "b9ca075cf6e845acade4af4446a82d1c"
          ]
        },
        "id": "n5REYkyWqlXi",
        "outputId": "ae4b4f49-8a52-4d31-8310-245efcfc7306"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NUM_PATCHES 27556\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "166e1a0a983649aa8dcf4983083b5d70",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Similarity Calculation:   0%|          |"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# B. Find cosine similarity between the eigen value vectors of the two images\n",
        "similarities = np.zeros((NUM_PATCHES, NUM_PATCHES), dtype=np.float32)\n",
        "print('NUM_PATCHES', NUM_PATCHES)\n",
        "input_image_local = tf.squeeze(input_image)\n",
        "# eig_flat_fig = plt.figure()\n",
        "\n",
        "for i in tqdm(range(NUM_IMAGES), desc=\"Similarity Calculation\", bar_format='{l_bar}{bar}|'):\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # model.add(SVLayer(PATCH_SIZE, STRIDE))\n",
        "    model.add(SVLayer(PATCH_SIZE, STRIDE, PATCH_SIZE_2, STRIDE_2))\n",
        "    single_img = original_imgs[i]\n",
        "    single_img = tf.expand_dims(single_img, axis=(0))\n",
        "    expanded_original_img = tf.expand_dims(single_img, axis=(3))\n",
        "\n",
        "    original_eigvals = model(expanded_original_img)\n",
        "    # original_eigvals = original_eigvals[0, :, :, :]\n",
        "\n",
        "    # Convert to unit vector\n",
        "    #######print('original_eigvals_x', original_eigvals.shape)\n",
        "    original_eigvals = original_eigvals / (\n",
        "                np.sqrt(np.sum(original_eigvals ** 2, axis=(5), keepdims=True)) + CORRECTION)\n",
        "\n",
        "    #######print('original_eigvals', original_eigvals.shape)\n",
        "\n",
        "    # Generate eigenvalues for the rotated image\n",
        "    single_rotated_img = rotated_imgs[i]\n",
        "    single_rotated_img = tf.expand_dims(single_rotated_img, axis=(0))\n",
        "    expanded_rotated_img = tf.expand_dims(single_rotated_img, axis=(3))\n",
        "    rotated_eigvals = model(expanded_rotated_img)\n",
        "    # rotated_eigvals = rotated_eigvals[0, :, :, :]\n",
        "\n",
        "    # Convert to unit vector\n",
        "    rotated_eigvals = rotated_eigvals / (\n",
        "                np.sqrt(np.sum(rotated_eigvals ** 2, axis=(5), keepdims=True)) + CORRECTION)\n",
        "\n",
        "    original_eigvals = np.reshape(original_eigvals,\n",
        "                                    (BIG_PATCHES * SMALL_PATCHES, original_eigvals.shape[-1]))\n",
        "\n",
        "    rotated_eigvals = np.reshape(rotated_eigvals, (BIG_PATCHES * SMALL_PATCHES, rotated_eigvals.shape[-1]))\n",
        "\n",
        "    similarities += np.matmul(original_eigvals, rotated_eigvals.T)\n",
        "\n",
        "    #print('ss', similarities.shape)\n",
        "\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "FK-ptR7nrB_n",
        "outputId": "4ea15d6c-a20d-4f99-b156-873c6b960d06"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'similarities' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-5508b0d267d3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# C. Generate Mapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarities\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mNUM_IMAGES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m expected_img = ndimage.affine_transform(input_image_local,\n\u001b[1;32m      4\u001b[0m                                         \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                         \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSCALE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSCALE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'similarities' is not defined"
          ]
        }
      ],
      "source": [
        "# C. Generate Mapping\n",
        "mapping = get_mapping(similarities / NUM_IMAGES)\n",
        "expected_img = ndimage.affine_transform(input_image_local,\n",
        "                                        transform,\n",
        "                                        output_shape=(SCALE * 28, SCALE * 28),\n",
        "                                        cval=0.0)\n",
        "\n",
        "##expected_img = ndimage.rotate(original_img, ROTATION, reshape=False, cval=0)\n",
        "\n",
        "params = {\n",
        "    'PATCH_SIZE': PATCH_SIZE,\n",
        "    'PATCH_SIZE_2': PATCH_SIZE_2,\n",
        "    'STRIDE': STRIDE,\n",
        "    'STRIDE_2': STRIDE_2,\n",
        "    'BIG_X_PATCHES': BIG_X_PATCHES,\n",
        "    'BIG_Y_PATCHES': BIG_Y_PATCHES,\n",
        "    'SMALL_X_PATCHES': SMALL_X_PATCHES,\n",
        "    'SMALL_Y_PATCHES': SMALL_Y_PATCHES,\n",
        "    'BIG_PATCHES': BIG_PATCHES,\n",
        "    'SMALL_PATCHES': SMALL_PATCHES,\n",
        "    'NUM_PATCHES': NUM_PATCHES\n",
        "}\n",
        "\n",
        "# # Mask for debugging which ppixels are moving where\n",
        "# mask = np.ones_like(input_img)\n",
        "# STRIP_W = 5\n",
        "# offset = -20\n",
        "# mask[:, W // 2 - STRIP_W + offset: W // 2 + STRIP_W + offset] = 1\n",
        "\n",
        "predicted_img = apply_mapping(input_image_local, mapping, params)\n",
        "\n",
        "# Display result of transformation\n",
        "result_fig = plt.figure()\n",
        "\n",
        "result_fig.add_subplot(2, 3, 1)\n",
        "input_image_local = tf.squeeze(input_image_local)\n",
        "plt.imshow(input_image_local)\n",
        "plt.axis('off')\n",
        "plt.title('Input Image')\n",
        "\n",
        "# result_fig.add_subplot(1, 4, 2)\n",
        "# plt.imshow(mask)\n",
        "# plt.axis('off')\n",
        "# plt.title('Mask')\n",
        "\n",
        "# result_fig.add_subplot(1, 4, 3)\n",
        "# plt.imshow(mask * input_img)\n",
        "# plt.axis('off')\n",
        "# plt.title('Mask & Input')\n",
        "\n",
        "result_fig.add_subplot(2, 3, 2)\n",
        "predicted_img = tf.squeeze(predicted_img)\n",
        "plt.imshow(predicted_img)\n",
        "plt.axis('off')\n",
        "plt.title('Predicted')\n",
        "\n",
        "\n",
        "filter_img = ndimage.grey_opening(predicted_img, size=(5, 5))\n",
        "filter_img = ndimage.grey_closing(filter_img, size=(5, 5))\n",
        "\n",
        "result_fig.add_subplot(2, 3, 3)\n",
        "plt.imshow(filter_img)\n",
        "plt.axis('off')\n",
        "plt.title('Filtered')\n",
        "\n",
        "mse_val, ssim_val = compare_images(predicted_img, expected_img)\n",
        "\n",
        "plot_data = {\n",
        "    'SCALE': [SCALE],\n",
        "    'NUM_IMAGES': [NUM_IMAGES],\n",
        "    'TRAIN_CLASS': [TRAIN_CLASS],\n",
        "    'TEST_CLASS': [FINAL_TEST_CLASS],\n",
        "    'NUM_PATCHES': [NUM_PATCHES],\n",
        "    'PREDICTION MSE': [f\"{mse_val:.2f}\"],\n",
        "    'PREDICTION SIMILARITY (%)': [f\"{ssim_val:.2f}\"]\n",
        "}\n",
        "\n",
        "table_data = [plot_data[key] for key in plot_data]\n",
        "table_columns = list(plot_data.keys())\n",
        "\n",
        "ax_table = result_fig.add_subplot(2, 3, 5)\n",
        "table = ax_table.table(cellText=table_data, rowLabels=table_columns, loc='center')\n",
        "ax_table.axis('off')\n",
        "\n",
        "result_fig.add_subplot(2, 3, 6)\n",
        "expected_img = tf.squeeze(expected_img)\n",
        "\n",
        "plt.imshow(expected_img)\n",
        "plt.axis('off')\n",
        "plt.title('Expected')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "16aadefb829a415e8425985dee3714a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27dd6b05b425425faaf154ebf5fc4e60": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d9e6c8f451f43099c7f74abb6ad0eff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e7a07c3bcfcf443699ba77f00a13cb14",
              "IPY_MODEL_dca3ba79a4cf4dafb15c82a01f2afdfa",
              "IPY_MODEL_e8bc24bfbfa342169755c8a8655f8ced"
            ],
            "layout": "IPY_MODEL_27dd6b05b425425faaf154ebf5fc4e60"
          }
        },
        "529f1b4cac4c45d797d47ba1e080b7ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67f2015f06e84ab88f7d34baad053af0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8284d15c5df24c74b8b3d80768a675f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95234a15648141f8a1e5edb04b968119": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b9ca075cf6e845acade4af4446a82d1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dca3ba79a4cf4dafb15c82a01f2afdfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67f2015f06e84ab88f7d34baad053af0",
            "max": 784,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_95234a15648141f8a1e5edb04b968119",
            "value": 784
          }
        },
        "e7a07c3bcfcf443699ba77f00a13cb14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_529f1b4cac4c45d797d47ba1e080b7ea",
            "placeholder": "",
            "style": "IPY_MODEL_8284d15c5df24c74b8b3d80768a675f1",
            "value": "Similarity Calculation: 100%"
          }
        },
        "e8bc24bfbfa342169755c8a8655f8ced": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16aadefb829a415e8425985dee3714a3",
            "placeholder": "",
            "style": "IPY_MODEL_b9ca075cf6e845acade4af4446a82d1c",
            "value": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
